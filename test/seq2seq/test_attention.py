#!/usr/bin/env python3


# hidden = cudavec(np.zeros((300,1,1024), dtype=np.float32))
# general = Attn('general', 1024).cuda(gpu_id)
# print(general)
# T.cuda.synchronize()
# t = time.time()
# context = general(hidden, y)
# T.cuda.synchronize()
# # print(context)
# print('General: Attention weights should be of size batch_size * 1 * max_len', context.size() == T.Size([300,1,12]), 'time taken', time.time()-t)
# dot = Attn('dot', 1024).cuda(gpu_id)
# print(dot)
# T.cuda.synchronize()
# t = time.time()
# context = dot(hidden, o)
# T.cuda.synchronize()
# print('Dot: Attention weights should be of size batch_size * 1 * max_len', context.size() == T.Size([300,1,12]), 'time taken', time.time()-t)
# att = Attn('concat', 1024).cuda(gpu_id)
# print(att)
# T.cuda.synchronize()
# t = time.time()
# context = att(hidden, o)
# T.cuda.synchronize()
# print('Concat: Attention weights should be of size batch_size * 1 * max_len', context.size() == T.Size([300,1,12]), 'time taken', time.time()-t)
